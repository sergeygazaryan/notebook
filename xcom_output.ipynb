{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import logging\n",
        "from pyspark.sql import SparkSession\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        ")\n",
        "logger = logging.getLogger('exampleLogger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize SparkSession with log level set to INFO\n",
        "spark = SparkSession.builder.appName(\"RDD Example from Array\").getOrCreate()\n",
        "spark.sparkContext.setLogLevel(\"INFO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example data array (Python list)\n",
        "data_array = [\"Apache Spark\", \"is\", \"a unified analytics engine\", \"for large-scale data processing.\"]\n",
        "\n",
        "# Parallelize the data array to create an RDD (Resilient Distributed Dataset)\n",
        "rdd = spark.sparkContext.parallelize(data_array)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Count the number of elements (items) in the RDD\n",
        "num_elements = rdd.count()\n",
        "logger.info(f\"Number of elements in the RDD: {num_elements}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Perform additional transformations and actions on the RDD\n",
        "word_counts = rdd.flatMap(lambda line: line.split(\" \")).count()\n",
        "logger.info(f\"Total number of words in the RDD: {word_counts}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stop the SparkSession\n",
        "spark.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/json": {
              "num_elements": 4,
              "word_counts": 9
            }
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Push metrics to XCom\n",
        "output = {'num_elements': num_elements, 'word_counts': word_counts}\n",
        "print(json.dumps(output))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e1a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import socket\n",
    "import os\n",
    "\n",
    "# Replace these with your actual values or fetch from environment variables\n",
    "KUBERNETES_SERVICE_ACCOUNT = 'airflow-worker',
    "NUM_EXECUTORS = int(os.getenv('NUM_EXECUTORS', '4'))\n",
    "EXECUTOR_CORES = int(os.getenv('EXECUTOR_CORES', '4'))\n",
    "EXECUTOR_MEMORY = os.getenv('EXECUTOR_MEMORY', '4g')\n",
    "DRIVER_MEMORY = os.getenv('DRIVER_MEMORY', '4g')\n",
    "SPARK_IMAGE = os.getenv('SPARK_IMAGE', 'your-spark-image')\n",
    "KUBERNETES_NAMESPACE = 'airflow',
    "YUNIKORN_QUEUE = os.getenv('YUNIKORN_QUEUE', 'your-queue')\n",
    "S3_ACCESS_KEY = os.getenv('S3_ACCESS_KEY', 'your-s3-access-key')\n",
    "S3_SECRET_KEY = os.getenv('S3_SECRET_KEY', 'your-s3-secret-key')\n",
    "S3_ENDPOINT = os.getenv('S3_ENDPOINT', 'your-s3-endpoint')\n",
    "EXECUTOR_LABEL = os.getenv('EXECUTOR_LABEL', 'your-executor-label')\n",
    "\n",
    "try:\n",
    "    driver_host = socket.gethostbyname(socket.gethostname())\n",
    "except Exception as e:\n",
    "    print(f\"Error getting driver host: {e}\")\n",
    "    driver_host = \"localhost\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(f\"{KUBERNETES_SERVICE_ACCOUNT}-{NUM_EXECUTORS}e-{EXECUTOR_CORES}c-{EXECUTOR_MEMORY}\") \\\n",
    "    .master(\"k8s://https://kubernetes.default.svc.cluster.local:443\") \\\n",
    "    .config(\"mapreduce.fileoutputcommitter.algorithm.version\", 2) \\\n",
    "    .config(\"mapreduce.fileoutputcommitter.marksuccessfuljobs\", \"false\") \\\n",
    "    .config(\"spark.kubernetes.container.image\", SPARK_IMAGE) \\\n",
    "    .config(\"spark.kubernetes.container.image.pullPolicy\", \"Always\") \\\n",
    "    .config(\"spark.kubernetes.authenticate.driver.serviceAccountName\", KUBERNETES_SERVICE_ACCOUNT) \\\n",
    "    .config(\"spark.kubernetes.authenticate.executor.serviceAccountName\", KUBERNETES_SERVICE_ACCOUNT) \\\n",
    "    .config(\"spark.kubernetes.namespace\", KUBERNETES_NAMESPACE) \\\n",
    "    .config(\"spark.kubernetes.scheduler.name\", \"yunikorn\") \\\n",
    "    .config(\"spark.kubernetes.executor.label.queue\", YUNIKORN_QUEUE) \\\n",
    "    .config(\"spark.kubernetes.executor.limit.cores\", EXECUTOR_CORES) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.access.key\", S3_ACCESS_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.secret.key\", S3_SECRET_KEY) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.endpoint\", S3_ENDPOINT) \\\n",
    "    .config(\"spark.hadoop.fs.s3a.path.style.access\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.connection.ssl.enabled\", \"false\") \\\n",
    "    .config(\"spark.hadoop.fs.s3a.impl\", \"org.apache.hadoop.fs.s3a.S3AFileSystem\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"s3a://intent_tmp/warehouse\") \\\n",
    "    .config(\"spark.executor.instances\", NUM_EXECUTORS) \\\n",
    "    .config(\"spark.executor.cores\", EXECUTOR_CORES) \\\n",
    "    .config(\"spark.executor.memory\", EXECUTOR_MEMORY) \\\n",
    "    .config(\"spark.driver.memory\", DRIVER_MEMORY) \\\n",
    "    .config(\"spark.kubernetes.executor.node.selector.executor\", EXECUTOR_LABEL) \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"10g\") \\\n",
    "    .config(\"spark.driver.host\", driver_host) \\\n",
    "    .config(\"spark.rpc.askTimeout\", 36000) \\\n",
    "    .config(\"spark.extraListeners\", \"sparkmonitor.listener.JupyterSparkMonitorListener\") \\\n",
    "    .config(\"spark.driver.extraClassPath\", \"/opt/conda/lib/python3.11/site-packages/sparkmonitor/listener_2.12.jar\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Your existing code...\n",
    "data_array = [\"Apache Spark\", \"is\", \"a unified analytics engine\", \"for large-scale data processing.\"]\n",
    "rdd = spark.sparkContext.parallelize(data_array)\n",
    "num_elements = rdd.count()\n",
    "print(f\"Number of elements in the RDD: {num_elements}\")\n",
    "num_words = rdd.flatMap(lambda line: line.split(\" \")).count()\n",
    "print(f\"Number of words in the RDD: {num_words}\")\n",
    "\n",
    "# Stop the SparkSession\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
